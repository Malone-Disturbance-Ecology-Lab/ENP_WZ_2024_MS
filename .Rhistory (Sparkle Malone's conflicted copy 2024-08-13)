# met data
load("SE1_Data_AllData.RDATA") # se1
# prepare se1 data to be one dataframe
se1$TIMESTAMP <- as.POSIXct(se1$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
df <- se1 %>% subset(date>= "2021-01-01" &
date< "2022-01-01") %>%
as.data.frame()
se.wl.df$TIMESTAMP <- as.POSIXct(se.wl.df$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
wl_df <- se.wl.df %>%
subset(TIMESTAMP>=  as.POSIXct("2021-01-01 00:00:00") &
TIMESTAMP< as.POSIXct("2022-01-01 00:00:00")) %>% as.data.frame()  %>% distinct(TIMESTAMP, .keep_all = TRUE) %>% mutate(wl = wl.corr) %>% select(TIMESTAMP, wl)
# only pull out variables we need
df2 <- df %>% dplyr::select("TIMESTAMP",  "NEE.filtered",  "TA.f", "PAR.f") %>% distinct(TIMESTAMP, .keep_all = TRUE)
# merge water level into df2
df2 <- left_join(df2, wl_df, by = "TIMESTAMP")
# Clear environment
rm(list = ls())
# load packages
library(tidyverse)   # everything
setwd("/Users/sm3466/YSE Dropbox/Sparkle Malone/Research/ENP_WZ_2024_MS/data/data_raw")
# water level
se.wl.df <- read.csv('SE_WaterLevel_2020_2022.csv') # se.wl.df
# met data
load("SE1_Data_AllData.RDATA") # se1
# prepare se1 data to be one dataframe
se1$TIMESTAMP <- as.POSIXct(se1$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
df <- se1 %>% subset(date>= "2021-01-01" &
date< "2022-01-01") %>%
as.data.frame()
se.wl.df$TIMESTAMP <- as.POSIXct(se.wl.df$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
wl_df <- se.wl.df %>%
subset(TIMESTAMP>=  as.POSIXct("2021-01-01 00:00:00") &
TIMESTAMP< as.POSIXct("2022-01-01 00:00:00")) %>% as.data.frame()  %>% distinct(TIMESTAMP, .keep_all = TRUE) %>% mutate(wl = wl.corr) %>% select(TIMESTAMP, wl)
# only pull out variables we need
df2 <- df %>% dplyr::select("TIMESTAMP",  "NEE.filtered",  "TA.f", "PAR.f") %>% distinct(TIMESTAMP, .keep_all = TRUE)
setwd("/Users/sm3466/YSE Dropbox/Sparkle Malone/Research/ENP_WZ_2024_MS/data/data_raw")
# met data
load("SE1_Data_AllData.RDATA") # se1
# prepare se1 data to be one dataframe
se1$TIMESTAMP <- as.POSIXct(se1$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
df <- se1 %>% subset(date>= "2021-01-01" &
date< "2022-01-01") %>%
as.data.frame()
se.wl.df <- read.csv('SE_WaterLevel_2020_2022.csv') # se.wl.df
se.wl.df$TIMESTAMP <- as.POSIXct(se.wl.df$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
wl_df <- se.wl.df %>%
subset(TIMESTAMP>=  as.POSIXct("2021-01-01 00:00:00") &
TIMESTAMP< as.POSIXct("2022-01-01 00:00:00")) %>% as.data.frame()  %>% distinct(TIMESTAMP, .keep_all = TRUE) %>% mutate(wl = wl.corr) %>% select(TIMESTAMP, wl)
# only pull out variables we need
df2 <- df %>% dplyr::select("TIMESTAMP",  "NEE.filtered",  "TA.f", "PAR.f") %>% distinct(TIMESTAMP, .keep_all = TRUE)
# merge water level into df2
df2 <- left_join(df2, wl_df, by = "TIMESTAMP")
df2$TIMESTAMP<- format(df2$TIMESTAMP, "%Y-%m-%d %H:%M:%S")
# save as csv like other station datasets
write.csv(df2, "AR_SE1_2021.csv", row.names = F)
rm(list = ls())
TSPH1 <- read.csv("AR_TSPH1_2021.csv")
SE1 <- read.csv("AR_SE1_2021.csv")
TSPH7 <- read.csv("AR_TSPH7_2021.csv")
SE1
TSPH1 <- TSPH1[-c(1)]
TSPH7 <- TSPH7[-c(1)]
# rename columns so there are no duplicate names for variables when merging the files
names(TSPH1) <- c("TIMESTAMP", "TS1.NEE.filtered", "TS1.TA.f", "TS1.PAR.f", "TS1.wl")
names(SE1) <- c("TIMESTAMP", "SE1.NEE.filtered", "SE1.TA.f", "SE1.PAR.f", "SE1.wl")
names(TSPH7) <- c("TIMESTAMP", "TS7.NEE.filtered", "TS7.TA.f", "TS7.PAR.f")
# convert TIMESTAMP to POSIXct
lapply(TSPH1, class) # SE1, TSPH7 # all TIMESTAMPS are character
TSPH1$TIMESTAMP <- as.POSIXct(TSPH1$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
SE1$TIMESTAMP <- as.POSIXct(SE1$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
TSPH7$TIMESTAMP <- as.POSIXct(TSPH7$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
# TSPH1 - average duplicate water level values together
which(duplicated(TSPH1$TIMESTAMP)) # duplicated entries because 2 water level values associated with every time point until 1198
TS1.wl.avg <- TSPH1 %>%
group_by(TIMESTAMP) %>%
summarize(mean(TS1.wl))
TSPH1 <- left_join(TSPH1, TS1.wl.avg, by = "TIMESTAMP") # add averaged water levels back into TSPH1 dataframe
# drop duplicates based on TIMESTAMP
TSPH1 <- TSPH1[!duplicated(TSPH1$TIMESTAMP), ]
# drop the original duplicated water level column
TSPH1 <- TSPH1[-c(5)]
# rename the averaged water level column to TS1.wl
names(TSPH1) <- c("TIMESTAMP", "TS1.NEE.filtered", "TS1.TA.f", "TS1.PAR.f", "TS1.wl")
rm(TS1.wl.avg)
# SE1 - add missing datetime back in and remove duplicates for other duplicated values
which(duplicated(SE1$TIMESTAMP))
# TSPH7 - average duplicated values
which(duplicated(TSPH7$TIMESTAMP)) # entries 14880 - 14883 are duplicated with different values for NEE and TA.f
TS7.NEE.avg <- TSPH7 %>%
group_by(TIMESTAMP) %>%
summarize(mean(TS7.NEE.filtered))
TS7.TA.avg <- TSPH7 %>%
group_by(TIMESTAMP) %>%
summarize(mean(TS7.TA.f))
TSPH7 <- left_join(left_join(TSPH7, TS7.NEE.avg, by = "TIMESTAMP"), TS7.TA.avg, by = "TIMESTAMP") # add averaged water levels back into TSPH7 dataframe
# drop duplicates based on TIMESTAMP
TSPH7 <- TSPH7[!duplicated(TSPH7$TIMESTAMP), ]
# drop the original duplicated columns
TSPH7 <- TSPH7[-c(2,3)]
# reorder columns to match others
TSPH7 <- TSPH7[,c(1,3,4,2)]
# rename the averaged water level column to TS1.wl
names(TSPH7) <- c("TIMESTAMP", "TS7.NEE.filtered", "TS7.TA.f", "TS7.PAR.f")
rm(TS7.NEE.avg, TS7.TA.avg)
Mangrove_wl <- read.csv("PHY_Castaneda_001.csv")
TSPH7_wl <- Mangrove_wl %>%
filter(SITENAME == "TS/Ph7") %>% # only TS7
filter(Date >= "2021") %>% # only 2021
mutate("TS7_hourly_wl" = WaterLevel*0.01) %>% # convert the WaterLevel units from centimeters to meters
as.data.frame()
# convert the variables to the proper class
TSPH7_wl$Date <- as.Date(TSPH7_wl$Date, format = "%Y-%m-%d")
# two ways to merge the water level data into the big data freme: via TIMESTAMP2 or via Date and Hour
# make a TIMESTAMP2 variable (so that don't have duplicate named variables when merging the dataframes)
TSPH7_wl$TIMESTAMP2 <- as.POSIXct(paste(TSPH7_wl$Date, TSPH7_wl$Time),
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
TSPH7_wl$Hour <- lubridate::hour(TSPH7_wl$TIMESTAMP2)
TSPH7_wl$Hour <- as.numeric(TSPH7_wl$Hour)
lapply(TSPH7_wl, class)
# only select the variables we need
TSPH7_wl <- TSPH7_wl[,c(7, 2, 8, 6)]
# merge files together into one dataframe based on "TIMESTAMP" ####################
WZ_df <- left_join(left_join(TSPH1, SE1, by = "TIMESTAMP"), TSPH7, by = "TIMESTAMP")
which(duplicated(WZ_df$TIMESTAMP))
# make a date column from TIMESTAMP
WZ_df$Date <- as.Date(as.character(WZ_df$TIMESTAMP), format = "%Y-%m-%d")
# make an Hour column from TIMESTAMP
WZ_df$Hour <- lubridate::hour(WZ_df$TIMESTAMP)
# merge TS7 water level by c(date, hour)
WZ_df <- left_join(WZ_df, TSPH7_wl, by = c("Date", "Hour"))  # this way duplicates , but gives the proper values
# summarize hourly water level for TS1 and SE1 and add to the WZ_df dataframe
hourly_wl <- WZ_df %>%
group_by(Date, Hour) %>%
summarize(mean(TS1.wl, na.rm = T),
mean(SE1.wl, na.rm =T))
names(hourly_wl) <- c("Date", "Hour", "TS1_hourly_wl", "SE1_hourly_wl")
# summarize daily water level for all sites and add to the WZ_df dataframe
mean_daily_wl <- WZ_df %>%
group_by(Date) %>%
summarize(mean(TS1.wl, na.rm = T),
mean(SE1.wl, na.rm = T),
mean(TS7_hourly_wl, na.rm = T))
names(mean_daily_wl) <- c("Date", "TS1_daily_wl", "SE1_daily_wl", "TS7_daily_wl" )
WZ_df <- left_join(left_join(WZ_df, hourly_wl, by = c("Date", "Hour")), mean_daily_wl, by = "Date")  # add hourly and daily averaged water levels back into WZ_df
WZ_df <- WZ_df[-c(15)] # drop the TIMESTAMP2 variable
WZ_df <- WZ_df %>% mutate_if(is.numeric ,~ifelse(is.nan(.), NA, .)) # converts NaN to NA
WZ_df <- WZ_df %>% mutate_if(is.numeric, ~ifelse(is.infinite(.), NA, .)) # converts Inf to NA
# reorder columns
names(WZ_df)
WZ_df <- WZ_df[,c(1,13,14,2:4,6:8,10:12,5,9,16,17,15,18:20)]
summary(WZ_df)
# fill any non-filled data in environmental variables (PAR and TA) ####
summary(WZ_df$TS7.PAR.f) # fill 4 NAs in TS7 PAR and TA as the average between TS1 and SE1
TS1_PAR <- WZ_df$TS1.PAR.f[is.na(WZ_df$TS7.PAR.f)]
SE1_PAR <- WZ_df$SE1.PAR.f[is.na(WZ_df$TS7.PAR.f)]
TS7_PAR <- rowMeans(cbind(TS1_PAR,SE1_PAR))
WZ_df$TS7.PAR.f[is.na(WZ_df$TS7.PAR.f)] <- TS7_PAR
summary(WZ_df$TS7.PAR.f)
TS1_TA <- WZ_df$TS1.TA.f[is.na(WZ_df$TS7.TA.f)]
SE1_TA <- WZ_df$SE1.TA.f[is.na(WZ_df$TS7.TA.f)]
TS7_TA <- rowMeans(cbind(TS1_TA,SE1_TA))
WZ_df$TS7.TA.f[is.na(WZ_df$TS7.TA.f)] <- TS7_TA
summary(WZ_df$TS7.TA.f)
rm(TS1_PAR, SE1_PAR, TS7_PAR, TS1_TA, SE1_TA,TS7_TA)
# TS1: canopy height = 0.73m (Zhao, 2019; Jimenez, 2012); 1.07m (early 2022 field measures)
#42*2.54 # 42 inches *2.54 = 106.68 cm
summary(WZ_df$TS1_daily_wl) # min is -0.428 ; max is 0.539, 384 NAs
# create indicator variable for water level for each site
WZ_df$TS1WLindicator <- NA
WZ_df$TS1WLindicator[WZ_df$TS1_daily_wl <=0] <- 0  #"dry" # below  soil surface
WZ_df$TS1WLindicator[WZ_df$TS1_daily_wl > 0 & WZ_df$TS1_daily_wl  <= 0.2675] <- 0.25  #"25% coverage"
WZ_df$TS1WLindicator[WZ_df$TS1_daily_wl > 0.2675 & WZ_df$TS1_daily_wl <= 0.535] <- 0.50 #"50% coverage"
WZ_df$TS1WLindicator[WZ_df$TS1_daily_wl > 0.535] <- 0.75 #">50% coverage"
class(WZ_df$TS1WLindicator)
WZ_df$TS1WLindicator <- as.factor(WZ_df$TS1WLindicator)
summary(WZ_df$TS1WLindicator)
# SE1: canopy height = 0.84 m (early 2022 field measures)
#33*2.54 # 83.82 cm ;  mean canopy height
#15*2.54 # 38.1 cm ; sawgrass canopy height
#40*2.54 # 101.6 cm ; mangroves canopy height
summary(WZ_df$SE1_daily_wl) # min is 0.373 ; max is 0.822
WZ_df$SE1WLindicator <- NA
WZ_df$SE1WLindicator[WZ_df$SE1_daily_wl > 0.21 & WZ_df$SE1_daily_wl <= 0.42] <- 0.50 #"50% coverage"
WZ_df$SE1WLindicator[WZ_df$SE1_daily_wl > 0.42 & WZ_df$SE1_daily_wl <= 0.63] <- 0.75 # "75% coverage"
WZ_df$SE1WLindicator[WZ_df$SE1_daily_wl > 0.63] <- 1 # "75% - 98% coverage"
WZ_df$SE1WLindicator <- as.factor(WZ_df$SE1WLindicator)
summary(WZ_df$SE1WLindicator)
# TS7: canopy height = 1.5m (Hogan, 2021; Ewe, 2006)
summary(WZ_df$TS7_daily_wl) # min is -0.06, max is 0.403 m
WZ_df$TS7WLindicator <- NA
WZ_df$TS7WLindicator[WZ_df$TS7_daily_wl <=0] <- 0 #"dry"
WZ_df$TS7WLindicator[WZ_df$TS7_daily_wl > 0 & WZ_df$TS7_daily_wl  <= 0.375] <- 0.25 # "0 - 25% coverage"
WZ_df$TS7WLindicator[WZ_df$TS7_daily_wl > 0.375 & WZ_df$TS7_daily_wl  <= 0.75] <- 0.50 # 25% - 27% coverage"
WZ_df$TS7WLindicator <- as.factor(WZ_df$TS7WLindicator)
summary(WZ_df$TS7WLindicator)
summary(WZ_df$SE1WLindicator )
# save as csv for later use ########################################################################
write.csv(WZ_df, "AR_flux_sites_2021.csv", row.names = FALSE)
# EOF
# EOF
# EOF
# EOF
# EOF
# EOF
# EOF
# EOF
getwd()
# load marl prairie data (TS1 water depth and surface salinity - 2021) ####
# load TS1 water level from the flux dataset (HOBO logger)
TSPH1_flux_met <- read.csv("data_raw/AR_TSPH1_2021.csv")
TSPH1_flux_met <- TSPH1_flux_met[-c(1)]
# Clear environment
rm(list = ls())
# load packages
library(tidyverse)
# load marl prairie data (TS1 water depth and surface salinity - 2021) ####
# load TS1 water level from the flux dataset (HOBO logger)
TSPH1_flux_met <- read.csv("data_raw/AR_TSPH1_2021.csv")
# load marl prairie data (TS1 water depth and surface salinity - 2021) ####
# load TS1 water level from the flux dataset (HOBO logger)
TSPH1_flux_met <- read.csv("data/data_raw/AR_TSPH1_2021.csv")
TS1_sal <- read_csv("data/data_raw/LT_ND_Rubio_001.csv")
getwd()
setwd('/Users/sm3466/YSE Dropbox/Sparkle Malone/Research/ENP_WZ_2024_MS/')
setwd('/Users/sm3466/YSE Dropbox/Sparkle Malone/Research/ENP_WZ_2024_MS')
# load marl prairie data (TS1 water depth and surface salinity - 2021) ####
# load TS1 water level from the flux dataset (HOBO logger)
TSPH1_flux_met <- read.csv("data/data_raw/AR_TSPH1_2021.csv")
TSPH1_flux_met <- TSPH1_flux_met[-c(1)]
TSPH1_flux_met$TIMESTAMP <- as.POSIXct(TSPH1_flux_met$TIMESTAMP,
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
TSPH1_flux_met$Date <- as.Date(TSPH1_flux_met$TIMESTAMP,
format = "%Y-%m-%d")
TSPH1_flux_wl <- TSPH1_flux_met %>%
group_by(Date) %>%
summarize(TS1_tower_daily_wl = mean(wl)) %>%
as.data.frame()
TSPH1_flux_wl$Week <- week(TSPH1_flux_wl$Date)
summary(TSPH1_flux_wl)
TS1_sal <- read_csv("data/data_raw/LT_ND_Rubio_001.csv")
lapply(TS1_sal, class)
TS1_sal$Date <- as.Date(TS1_sal$Date, format = "%Y-%m-%d")
TS1_sal$SITENAME <- as.factor(TS1_sal$SITENAME)
TS1_sal$Date <- as.Date(TS1_sal$Date, format = "%Y-%m-%d")
TS1_sal$SITENAME <- as.factor(TS1_sal$SITENAME)
TS1_sal <- TS1_sal %>%
filter(SITENAME == "TS/Ph1a") %>%
filter(Date > "2020-12-31" & Date <= "2021-12-31" ) # salinity data from 2003-03 to end of 2021
TS1_sal$Week <- week(TS1_sal$Date)
TS1_sal <- TS1_sal %>%
select(Date, Week, Salinity)
TS1_sal <- TS1_sal %>%
rename(TS1_mean_weekly_Sal = Salinity)
TS1_sal_21_w <- TS1_sal %>%
group_by(Week) %>%
summarize(TS1_mean_weekly_Sal= mean(TS1_mean_weekly_Sal, na.rm = TRUE))
TS1_wl_sal <- left_join(TSPH1_flux_wl, TS1_sal_21_w, by = "Week")
TS1_wl_sal <- TS1_wl_sal[-c(366),] # drop 2022-01-01
TS1_wl_sal <- TS1_wl_sal %>%
rename(date = Date)
# water level
load("data/data_raw/SE1_WaterLevel.RDATA") #last modified: 4/5/2022 # se.wl.df
lapply(se.wl.df, class)
# met data
load("data_raw/se1_Data_AllData.RDATA") # se1
# met data
load("data/data_raw/se1_Data_AllData.RDATA") # se1
# fix the timestamps and add a date column to se.wl.df
se1$TIMESTAMP <- as.POSIXct(se1$TIMESTAMP,
format= "%Y-%m-%d %H:%M:%S",
tz="EST")
se.wl.df$TIMESTAMP <- as.POSIXct(se.wl.df$TIMESTAMP,
format= "%Y-%m-%d %H:%M:%S",
tz="EST")
se.wl.df$date <- as.Date(as.character(se.wl.df$TIMESTAMP))
# prepare se1 data to be one dataframe #
df <- se1 %>%
subset(date >= "2021-01-01" &
date < "2022-01-01") %>%
as.data.frame()
wl_df <- se.wl.df %>%
subset (date >= "2021-01-01" &
date < "2022-01-01") %>%
as.data.frame()
which(duplicated(wl_df$TIMESTAMP)) # 14872 14874
wl_df <- wl_df[!duplicated(wl_df$TIMESTAMP), ]
# only pull out variables we need from the df dataframe
names(df)
df2 <- df %>% dplyr::select("TIMESTAMP",  "Year",  "Month", "Day", "Hour", "Doy", "Hour2", "date",
"EC", "Salinity.Hill1986")
names(df2)
# df2 has 22710, but should be 17520 observations --> drop duplicate timestamps in df2
which(duplicated(df2$TIMESTAMP)) # 2021-01-01 until 2021-01-13 14:00:00 are quadrupled
df2 <- df2[!duplicated(df2$TIMESTAMP), ]
# merge water level into df2
SE1_wl_sal <- left_join(df2, wl_df, by = "TIMESTAMP")
which(duplicated(SE1_wl_sal$TIMESTAMP))
SE1_wl_sal$TIMESTAMP <- as.POSIXct(SE1_wl_sal$TIMESTAMP,
format= "%Y-%m-%d %H:%M:%S",
tz="EST")
# filter EC and Sal values that are out of range and replace with NAs
which(SE1_wl_sal$EC==7999) #1648 7999's
SE1_wl_sal$EC.filtered <- SE1_wl_sal$EC
SE1_wl_sal$EC.filtered[SE1_wl_sal$EC==7999] <-NA
which(SE1_wl_sal$EC.filtered==7999)
SE1_wl_sal$Sal.Hill1986.filtered <- SE1_wl_sal$Salinity.Hill1986
SE1_wl_sal$Sal.Hill1986.filtered[SE1_wl_sal$EC==7999] <-NA
# rename the SE1 columns to include site code
names(SE1_wl_sal)
SE1_wl_sal <- SE1_wl_sal %>%
rename(date = date.x, SE1_EC = EC, SE1_wl = wl, SE1_SalHill = Salinity.Hill1986,
SE1_EC.filtered = EC.filtered, SE1_Sal.filtered = Sal.Hill1986.filtered)
# drop additional date column
SE1_wl_sal <- SE1_wl_sal[-c(12)] #"date.y"
# summarize daily SE1 water level and salinity statistics
se1daily21 <- SE1_wl_sal %>%
group_by(date) %>%
summarize(SE1_mean_daily_WL = mean(SE1_wl, na.rm = TRUE),
SE1_mean_daily_EC = mean(SE1_EC.filtered, na.rm = TRUE),
SE1_mean_daily_Sal = mean(SE1_Sal.filtered, na.rm = TRUE)) %>%
as.data.frame()
se1daily21 <- se1daily21 %>% mutate_if(is.numeric ,~ifelse(is.nan(.), NA, .)) # converts NaN to NA
se1daily21 <- se1daily21 %>% mutate_if(is.numeric, ~ifelse(is.infinite(.), NA, .)) # converts Inf to NA
# add the daily SE1 summary back into the big dataframe
SE1_wl_sal <- left_join(SE1_wl_sal, se1daily21, by = "date")
Mangrove_wl <- read.csv("data_raw/PHY_Castaneda_001.csv")
Mangrove_wl <- read.csv("data/data_raw/PHY_Castaneda_001.csv")
# convert the variables to the proper class
Mangrove_wl$Date <- as.Date(Mangrove_wl$Date, format = "%Y-%m-%d")
Mangrove_wl$SITENAME <- as.factor(Mangrove_wl$SITENAME)
TS7_wl <- Mangrove_wl %>%
filter(SITENAME == "TS/Ph7") %>% # only TS7
filter(Date >= "2021-01-01") %>% # only 2021
mutate("TS7_hourly_wl" = WaterLevel*0.01) %>% # convert the WaterLevel units from centimeters to meters
as.data.frame()
TS7_wl$TIMESTAMP <- as.POSIXct(paste(TS7_wl$Date, TS7_wl$Time),
format = "%Y-%m-%d %H:%M:%S",
tz = "EST")
lapply(TS7_wl, class)
TS7_wl$Hour <- lubridate::hour(TS7_wl$TIMESTAMP)
TS7_wl$Hour <- as.numeric(TS7_wl$Hour)
which(TS7_wl$TS7_hourly_wl <= -99.99) # no NA's
TS7_wl_daily <- TS7_wl %>%
group_by(Date) %>%
summarize(TS7_mean_daily_wl = mean(TS7_hourly_wl, na.rm = TRUE)) %>%
as.data.frame()
TS7_wl_daily <- TS7_wl_daily %>% mutate_if(is.numeric ,~ifelse(is.nan(.), NA, .)) # converts NaN to NA
TS7_wl_daily <- TS7_wl_daily %>% mutate_if(is.numeric, ~ifelse(is.infinite(.), NA, .)) # converts Inf to NA
TS7_wl_daily <- TS7_wl_daily %>%
rename(date = Date)
TS7_wl_2 <- TS7_wl %>%
select(TIMESTAMP, TS7_hourly_wl)
TS7_sal <- read_csv("data/data_raw/LT_ND_Losada_001-2.csv")
TS7_sal$SITENAME <- as.factor(TS7_sal$SITENAME)
TS7_sal_21 <- TS7_sal %>%
filter(SITENAME == "TS/Ph7a") %>%
filter(Date >= "2021-01-01" & Date <= "2021-12-31" )
TS7_sal_21$Week <- week(TS7_sal_21$Date)
# replace NA value identifier with NA
# find missing data (-9999) and convert to NA
which(TS7_sal_21$Salinity <= -9999) # 1 NA
TS7_sal_21$Salinity[TS7_sal_21$Salinity<= -9999] <-NA
TS7_sal_21_w <- TS7_sal_21 %>%
group_by(Week) %>%
summarize(TS7_mean_weekly_Sal= mean(Salinity, na.rm = TRUE))
# join all data together in a new dataframe ####
wl_sal_2021 <- left_join(SE1_wl_sal, TS1_wl_sal, by = "date")
wl_sal_2021 <- left_join(wl_sal_2021, TS7_wl_2, by = "TIMESTAMP")
wl_sal_2021 <- left_join(wl_sal_2021, TS7_wl_daily, by = "date")
wl_sal_2021 <- left_join(wl_sal_2021, TS7_sal_21_w, by = "Week")
# reorder columns by timestep resolution
names(wl_sal_2021)
wl_sal_2021<- wl_sal_2021[,c(1,8,2:3,18,4,6,5,7,17,19,11,9:10,12:16,20:22)]
# fill the NA's in the time columns (Mar 14, 2021 and Dec 31, 2021)
wl_sal_2021$Year[is.na(wl_sal_2021$Year)] <-"2021"
which(is.na(wl_sal_2021$Month)) # fill NA's differently to account for gap in March [3461, 3462]
wl_sal_2021$Month[3461:3462] <- "3"
wl_sal_2021$Month[17511:17520] <- "12"
which(is.na(wl_sal_2021$Day))
wl_sal_2021$Day[3461:3462] <- "14"
wl_sal_2021$Day[17511:17520] <- "31"
which(is.na(wl_sal_2021$Doy))
wl_sal_2021$Doy[3461:3462] <- "73"
wl_sal_2021$Doy[17511:17520] <- "365"
lapply(wl_sal_2021, class)
wl_sal_2021$Year <- as.numeric(wl_sal_2021$Year)
wl_sal_2021$Month <- as.numeric(wl_sal_2021$Month)
wl_sal_2021$Day <- as.numeric(wl_sal_2021$Day)
wl_sal_2021$Doy <- as.numeric(wl_sal_2021$Doy)
# weekly
weekly21 <- wl_sal_2021 %>%
group_by(Week) %>%
summarize(TS1_mean_weekly_WL = mean(TS1_tower_daily_wl, na.rm = TRUE),
SE1_mean_weekly_WL = mean(SE1_wl, na.rm = TRUE),
SE1_mean_weekly_EC = mean(SE1_EC.filtered, na.rm = TRUE),
SE1_mean_weekly_Sal = mean(SE1_Sal.filtered, na.rm = TRUE),
TS7_mean_weekly_WL = mean(TS7_hourly_wl, na.rm = TRUE)) %>%
as.data.frame()
weekly21 <- left_join(weekly21, TS1_sal_21_w, by = "Week")
weekly21 <- left_join(weekly21, TS7_sal_21_w, by = "Week")
weekly21 <- weekly21 %>% mutate_if(is.numeric ,~ifelse(is.nan(.), NA, .)) # converts NaN to NA
weekly21 <- weekly21 %>% mutate_if(is.numeric, ~ifelse(is.infinite(.), NA, .)) # converts Inf to NA
names(weekly21)
weekly21 <- weekly21[,c(1:2,7,3:6,8)]
# export weekly summary as csv ####
write.csv(weekly21, "data_processed/AR_wl_sal_2021_weekly.csv", row.names = FALSE)
# export weekly summary as csv ####
write.csv(weekly21, "data/AR_wl_sal_2021_weekly.csv", row.names = FALSE)
# export weekly summary as csv ####
write.csv(weekly21, "data/data_raw/AR_wl_sal_2021_weekly.csv", row.names = FALSE)
df <- read.csv( 'data/data_raw/AR_wl_sal_2021_weekly.csv')
df$lowWL <- 0
df$lowWL[df$TS7_mean_weekly_WL < -0.50]<- 1
df$lowWL <- as.factor(df$lowWL)
prior1 <-get_prior(bf(TS7_mean_weekly_Sal ~ s(TS7_mean_weekly_WL) + arma(p= 1, q=0) ) ,data = df,  family = gaussian())
tsph7 <- brm( bf(TS7_mean_weekly_Sal ~ s(TS7_mean_weekly_WL) + arma( p= 1, q=0) ) , prior = prior1,
data = df, backend = "cmdstanr",iter = 20000, family = gaussian(),
cores=10, seed=101, refresh = 0, thin=30, chains = 10,
control = list(adapt_delta = 0.99))
summary(tsph7)
pp_check(tsph7,ndraws = 100)
pp_check(tsph7, type = "ecdf_overlay")
mcmc_plot(tsph7, type = "pairs")
mcmc_plot(tsph7, type = "acf") # check for auto correlation issues
plot.tsph7 <- conditional_effects(tsph7, effects="TS7_mean_weekly_WL", prob = 0.89)
plot.tsph7.df  <- as.data.frame(plot.tsph7[[1]])
prior.se1 <- get_prior(bf(SE1_mean_weekly_Sal ~ s(SE1_mean_weekly_WL) + arma(p= 0, q=1, time=Week)),data = df,  family = gaussian())
# Date is notin the dfso i used week
se1 <- brm( bf(SE1_mean_weekly_Sal ~ s(SE1_mean_weekly_WL) + arma(p= 0, q=1)), prior = prior.se1 ,
data = df, family='gaussian', backend = "cmdstanr",iter = 20000, thin = 20,chains=10,
cores=10, seed=100, refresh = 0, warmup=10000,
control = list(adapt_delta = 0.99), normalize=T)
summary(se1)
pp_check(se1,ndraws = 100)
pp_check(se1, type = "ecdf_overlay",ndraws = 100)
mcmc_plot(se1, type = "pairs")
mcmc_plot(se1, type = "acf") # check for auto correlation issues
plot.se1 <- conditional_effects(se1, effects="SE1_mean_weekly_WL", prob = 0.89)
plot.se1.df  <- as.data.frame(plot.se1[[1]])
prior.ts1 <- get_prior(bf(TS1_mean_weekly_Sal ~ s(TS1_mean_weekly_WL) + arma(p= 0, q=1, time=Week)),data = df,  family = gaussian())
ts1 <- brm( bf(TS1_mean_weekly_Sal ~ s(TS1_mean_weekly_WL) + arma(p= 0, q=1)), prior = prior.ts1 ,
data = df, family='gaussian', backend = "cmdstanr",iter = 20000, thin = 20,chains=10,
cores=10, seed=100, refresh = 0, warmup=10000,
control = list(adapt_delta = 0.99), normalize=T)
summary(ts1)
pp_check(ts1,ndraws = 100)
pp_check(ts1, type = "ecdf_overlay",ndraws = 100)
mcmc_plot(ts1, type = "pairs")
mcmc_plot(ts1, type = "acf") # check for auto correlation issues
plot.ts1 <- conditional_effects(ts1, effects="TS1_mean_weekly_WL", prob = 0.89)
plot.ts1.df  <- as.data.frame(plot.ts1[[1]])
save( df, ts1, plot.ts1, plot.ts1.df,tsph7, plot.tsph7, se1, plot.se1, plot.se1.df, plot.tsph7.df, file="GAMs_Analysis_2024.RDATA"  )
# Figures:    #####
rm(list=ls())
load("~/YSE Dropbox/Sparkle Malone/Research/ENP_WZ_2024_MS/GAMs_Analysis_2024 (Sparkle Malone's conflicted copy 2024-08-12).RDATA")
tsph7.plot <- ggplot() +geom_point(data=df,
aes(TS1_mean_weekly_WL, TS1_mean_weekly_Sal) , alpha=0.3 , color="#000099")  +
theme_minimal() + xlab('Water Level (m)') + ylab('Salinity (PSU)') + theme(text = element_text(size = 18)) + ylim(0, 1 )
tsph7.plot <- ggplot() +
geom_line(data=plot.tsph7.df,
aes(TS7_mean_weekly_WL, estimate__), color="#43b284", size=1.5) +
geom_ribbon(data=plot.tsph7.df, aes(x=TS7_mean_weekly_WL, y=estimate__,
ymin=estimate__ + se__, ymax=estimate__- se__),
alpha=0.1, fill = "#43b284",  color="transparent") +
geom_point(data=df,
aes(TS7_mean_weekly_WL, TS7_mean_weekly_Sal) , alpha=0.3 , color="#43b284")  +
theme_minimal() + xlab('Water Level (m)') + ylab('Salinity (PSU)') +  theme(text = element_text(size = 18))
se1.plot <- ggplot() +
geom_line(data=plot.se1.df,
aes(SE1_mean_weekly_WL, estimate__), color="#fab255", size=1.5) +
geom_ribbon(data=plot.se1.df, aes(x=SE1_mean_weekly_WL, y=estimate__,
ymin=estimate__ + se__, ymax=estimate__- se__),
alpha=0.1, fill = "#fab255",  color="transparent") +
geom_point(data=df,
aes(SE1_mean_weekly_WL, SE1_mean_weekly_Sal) , alpha=0.3 , color="#fab255")  +
theme_minimal() + xlab('Water Level (m)') + ylab('Salinity (PSU)') + theme(text = element_text(size = 18))
ts1.plot <- ggplot() +
geom_line(data=plot.ts1.df,
aes(TS1_mean_weekly_WL, estimate__), color="#000099", size=1.5) +
geom_ribbon(data=plot.ts1.df, aes(x=TS1_mean_weekly_WL, y=estimate__,
ymin=estimate__ + se__, ymax=estimate__- se__),
alpha=0.1, fill = "#000099",  color="transparent") +
geom_point(data=df,
aes(TS1_mean_weekly_WL, TS1_mean_weekly_Sal) , alpha=0.3 , color="#000099")  +
theme_minimal() + xlab('Water Level (m)') + ylab('Salinity (PSU)') + theme(text = element_text(size = 18))   +
ylim(0,1) + xlim(0.3, 0.525)
library(ggpubr)
png(file="figures/WaterLvesvsSalinity_2024.png",
width=800, height=300)
ggarrange(ts1.plot, se1.plot,tsph7.plot,  ncol = 3, labels = c("A","B", "C"))
dev.off()
summary(tsph7)
